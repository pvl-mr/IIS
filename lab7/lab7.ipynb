{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Переход в директорию с обучающим текстом**"
      ],
      "metadata": {
        "id": "d1oJT9hh75rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/'Colab Notebooks'\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1G5MNCUW73k",
        "outputId": "e23dcca0-18a2-472e-a453-77fc09acb37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks\n",
            " onegin_english.txt\n",
            " onegin.txt\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            " Untitled2.ipynb\n",
            " weights-improvement-01-0.8797.hdf5\n",
            " weights-improvement-01-0.8880.hdf5\n",
            " weights-improvement-02-0.7154.hdf5\n",
            " weights-improvement-02-0.7202.hdf5\n",
            " weights-improvement-03-0.6646.hdf5\n",
            " weights-improvement-03-0.6775.hdf5\n",
            " weights-improvement-04-0.6312.hdf5\n",
            " weights-improvement-04-0.6405.hdf5\n",
            " weights-improvement-05-0.6130.hdf5\n",
            " weights-improvement-05-0.6149.hdf5\n",
            "'Копия блокнота \"Добро пожаловать в Colaboratory!\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Загрузка текста и кодировка в цифровое/символьное представление**"
      ],
      "metadata": {
        "id": "FvtkIYez8L6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import time\n",
        "\n",
        "\n",
        "filename = \"onegin_english.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n"
      ],
      "metadata": {
        "id": "Mz9uSUbiaz5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Обозначение последовательностей текста для обработки моделью**"
      ],
      "metadata": {
        "id": "TdavRlME8_lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i+seq_length]\n",
        "    seq_out = raw_text[i+seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "X = X/float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pds8GNYGa7kU",
        "outputId": "eedce182-0609-4dfe-9ee7-0f9ce926c9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  231055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epochs = 5. Легкая архитектура**"
      ],
      "metadata": {
        "id": "GQJ6qLul6gfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "mins = (time.time() - start_time)/60\n",
        "print(f\"Выполнялось:{round(mins, 2)} минут\")\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed: \")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"==============================\")\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l-TjdT5Wn1e",
        "outputId": "fd551f49-26da-45fa-e54a-c990b570bea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.7240\n",
            "Epoch 1: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 28s 14ms/step - loss: 2.7240\n",
            "Epoch 2/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.4686\n",
            "Epoch 2: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 26s 15ms/step - loss: 2.4687\n",
            "Epoch 3/5\n",
            "1804/1806 [============================>.] - ETA: 0s - loss: 2.4064\n",
            "Epoch 3: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 26s 14ms/step - loss: 2.4064\n",
            "Epoch 4/5\n",
            "1803/1806 [============================>.] - ETA: 0s - loss: 2.3753\n",
            "Epoch 4: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 26s 14ms/step - loss: 2.3753\n",
            "Epoch 5/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.3446\n",
            "Epoch 5: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 26s 14ms/step - loss: 2.3446\n",
            "Выполнялось:2.41 минут\n",
            "Seed: \n",
            "\" \n",
            "     i might have seen you, heard you speak\n",
            "     on visits to us, and in greeting\n",
            "     i might have \"\n",
            "==============================\n",
            " an the soaet      the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the soae  a doae the soae to saae\n",
            "     the so\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epochs = 5. Усложненная архитектура**"
      ],
      "metadata": {
        "id": "_kGEwztmg1i4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6lgJ10sFDpd",
        "outputId": "158f48ad-9959-46d4-cfa2-b49d6d55613c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1806/1806 [==============================] - ETA: 0s - loss: 2.6450\n",
            "Epoch 1: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 61s 32ms/step - loss: 2.6450\n",
            "Epoch 2/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.3697\n",
            "Epoch 2: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 57s 32ms/step - loss: 2.3697\n",
            "Epoch 3/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.2837\n",
            "Epoch 3: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.2837\n",
            "Epoch 4/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.2120\n",
            "Epoch 4: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.2120\n",
            "Epoch 5/5\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.1530\n",
            "Epoch 5: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.1529\n",
            "Выполнялось:4.87 минут\n",
            "Seed: \n",
            "\" st\n",
            "     comes for awakening, comes for slumber;\n",
            "     blessed are daytime's care and cark,\n",
            "     blest \"\n",
            "==============================\n",
            " the sooe a soue a sooeeted\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe a soue a sooee\n",
            "     the sooe the sooe\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "mins = (time.time() - start_time)/60\n",
        "print(f\"Выполнялось:{round(mins, 2)} минут\")\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed: \")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"==============================\")\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epochs 25. Усложненная архитектура**"
      ],
      "metadata": {
        "id": "GQcEV_XGpt11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=25, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "mins = (time.time() - start_time)/60\n",
        "print(f\"Выполнялось:{round(mins, 2)} минут\")\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed: \")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"==============================\")\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDogH9GteMmv",
        "outputId": "50f63db5-8623-443d-9ade-6964b19c8ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1804/1806 [============================>.] - ETA: 0s - loss: 2.6324\n",
            "Epoch 1: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 63s 33ms/step - loss: 2.6323\n",
            "Epoch 2/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.3607\n",
            "Epoch 2: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 57s 32ms/step - loss: 2.3607\n",
            "Epoch 3/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.2787\n",
            "Epoch 3: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.2787\n",
            "Epoch 4/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.2110\n",
            "Epoch 4: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 57s 32ms/step - loss: 2.2110\n",
            "Epoch 5/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.1472\n",
            "Epoch 5: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.1472\n",
            "Epoch 6/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.0885\n",
            "Epoch 6: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.0884\n",
            "Epoch 7/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.0400\n",
            "Epoch 7: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.0400\n",
            "Epoch 8/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9943\n",
            "Epoch 8: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 57s 32ms/step - loss: 1.9944\n",
            "Epoch 9/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9546\n",
            "Epoch 9: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.9546\n",
            "Epoch 10/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9183\n",
            "Epoch 10: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 57s 32ms/step - loss: 1.9183\n",
            "Epoch 11/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8873\n",
            "Epoch 11: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8873\n",
            "Epoch 12/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8585\n",
            "Epoch 12: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8585\n",
            "Epoch 13/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8310\n",
            "Epoch 13: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8311\n",
            "Epoch 14/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8081\n",
            "Epoch 14: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8081\n",
            "Epoch 15/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7830\n",
            "Epoch 15: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7830\n",
            "Epoch 16/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7611\n",
            "Epoch 16: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7612\n",
            "Epoch 17/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7408\n",
            "Epoch 17: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7409\n",
            "Epoch 18/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7203\n",
            "Epoch 18: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7203\n",
            "Epoch 19/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7053\n",
            "Epoch 19: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7053\n",
            "Epoch 20/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6866\n",
            "Epoch 20: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6866\n",
            "Epoch 21/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6714\n",
            "Epoch 21: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6715\n",
            "Epoch 22/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6579\n",
            "Epoch 22: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6579\n",
            "Epoch 23/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6451\n",
            "Epoch 23: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6451\n",
            "Epoch 24/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6302\n",
            "Epoch 24: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6302\n",
            "Epoch 25/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6147\n",
            "Epoch 25: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6147\n",
            "Выполнялось:24.14 минут\n",
            "Seed: \n",
            "\"  drops, hand softly sliding\n",
            "     to heart. and in his misted gaze\n",
            "     is death, not pain. so gently \"\n",
            "==============================\n",
            " seed.\n",
            "     {184}\n",
            "\n",
            "        xxiii\n",
            "\n",
            "     the words of she sale sane and shee,\n",
            "     and shen the sale of the dondession\n",
            "     of she sale sane of she sale saie\n",
            "     and she sale shene the soun to sane,\n",
            "     and she sale sane and she sale saie\n",
            "     and she sale sane and she sale saie\n",
            "     and she sale sane and she sale saie\n",
            "     and she sale sane and she sale saie\n",
            "     and she sale sane and she sale saie\n",
            "     and she sale sane and she sale saie\n",
            "     and she sale sane and she sale saie\n",
            "     and she sa\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epochs = 25. Усложненная архитектура с меньшим Dropoutом**"
      ],
      "metadata": {
        "id": "UKDa55Em7ixZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=25, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "mins = (time.time() - start_time)/60\n",
        "print(f\"Выполнялось:{round(mins, 2)} минут\")\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed: \")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"==============================\")\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr2XlvvX7xHw",
        "outputId": "5fa14e40-36db-4bbd-baa7-2b7d9e3e5c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1806/1806 [==============================] - ETA: 0s - loss: 2.6019\n",
            "Epoch 1: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 62s 33ms/step - loss: 2.6019\n",
            "Epoch 2/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.3316\n",
            "Epoch 2: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.3316\n",
            "Epoch 3/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.2346\n",
            "Epoch 3: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.2345\n",
            "Epoch 4/25\n",
            "1806/1806 [==============================] - ETA: 0s - loss: 2.1663\n",
            "Epoch 4: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 60s 33ms/step - loss: 2.1663\n",
            "Epoch 5/25\n",
            "1804/1806 [============================>.] - ETA: 0s - loss: 2.1033\n",
            "Epoch 5: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.1034\n",
            "Epoch 6/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.0505\n",
            "Epoch 6: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 59s 33ms/step - loss: 2.0505\n",
            "Epoch 7/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.0024\n",
            "Epoch 7: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.0025\n",
            "Epoch 8/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9629\n",
            "Epoch 8: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.9629\n",
            "Epoch 9/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9224\n",
            "Epoch 9: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.9224\n",
            "Epoch 10/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8895\n",
            "Epoch 10: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8895\n",
            "Epoch 11/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8570\n",
            "Epoch 11: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8570\n",
            "Epoch 12/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8256\n",
            "Epoch 12: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8256\n",
            "Epoch 13/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8011\n",
            "Epoch 13: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8011\n",
            "Epoch 14/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7738\n",
            "Epoch 14: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7738\n",
            "Epoch 15/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7509\n",
            "Epoch 15: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7509\n",
            "Epoch 16/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7301\n",
            "Epoch 16: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7301\n",
            "Epoch 17/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7098\n",
            "Epoch 17: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7098\n",
            "Epoch 18/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6900\n",
            "Epoch 18: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 59s 32ms/step - loss: 1.6900\n",
            "Epoch 19/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6714\n",
            "Epoch 19: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6715\n",
            "Epoch 20/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6560\n",
            "Epoch 20: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6560\n",
            "Epoch 21/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6367\n",
            "Epoch 21: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6368\n",
            "Epoch 22/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6218\n",
            "Epoch 22: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6218\n",
            "Epoch 23/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6062\n",
            "Epoch 23: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6062\n",
            "Epoch 24/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5915\n",
            "Epoch 24: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5915\n",
            "Epoch 25/25\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5782\n",
            "Epoch 25: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5782\n",
            "Выполнялось:24.25 минут\n",
            "Seed: \n",
            "\" r with my passion;\n",
            "     i'm yours, in a predestined fashion,\n",
            "     and i surrender to my fate.''\n",
            "     \"\n",
            "==============================\n",
            " {12}\n",
            "\n",
            "        xxxii\n",
            "\n",
            "     the sears and suream and she see stanted\n",
            "     and she seal the seroun the same,\n",
            "     and then the strengng with the same,\n",
            "     and shene a sort and sheht and still\n",
            "     and shene and suream and sheh a searon\n",
            "     and she seal the seroun the same,\n",
            "     and then the strengng with the same,\n",
            "     and shene a sort and sheht and still\n",
            "     and shene and suream and sheh a searon\n",
            "     and she seal the seroun the same,\n",
            "     and then the strengng with the same,\n",
            "     and shene a sort and sheht and still\n",
            "     and shene and suream and sheh a searon\n",
            "     and she seal the seroun the same,\n",
            "     and then the strengng with the same,\n",
            "     and shene a sort and sheht and still\n",
            "     and shene and suream and sheh a searon\n",
            "     and she seal the seroun the same,\n",
            "     and then the strengng with the same,\n",
            "     and shene a sort and sheht and still\n",
            "     and shene and suream and sheh a searon\n",
            "     and she seal the seroun the same,\n",
            "     and then the strengng with the same,\n",
            "     and shene a\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epochs = 50. Усложненная архитектура**"
      ],
      "metadata": {
        "id": "4jrDhi815IH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "mins = (time.time() - start_time)/60\n",
        "print(f\"Выполнялось:{round(mins, 2)} минут\")\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed: \")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"==============================\")\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jf5xZWbWnUN",
        "outputId": "71d223e1-e9ae-4e41-c823-788d724bb876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1804/1806 [============================>.] - ETA: 0s - loss: 2.6112\n",
            "Epoch 1: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 60s 32ms/step - loss: 2.6110\n",
            "Epoch 2/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.3539\n",
            "Epoch 2: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.3538\n",
            "Epoch 3/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.2622\n",
            "Epoch 3: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.2622\n",
            "Epoch 4/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.1891\n",
            "Epoch 4: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.1891\n",
            "Epoch 5/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.1292\n",
            "Epoch 5: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.1291\n",
            "Epoch 6/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.0768\n",
            "Epoch 6: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.0769\n",
            "Epoch 7/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.0299\n",
            "Epoch 7: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.0299\n",
            "Epoch 8/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9880\n",
            "Epoch 8: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.9880\n",
            "Epoch 9/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9523\n",
            "Epoch 9: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.9523\n",
            "Epoch 10/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.9196\n",
            "Epoch 10: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.9196\n",
            "Epoch 11/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8909\n",
            "Epoch 11: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8908\n",
            "Epoch 12/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8643\n",
            "Epoch 12: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8644\n",
            "Epoch 13/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8394\n",
            "Epoch 13: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8395\n",
            "Epoch 14/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.8158\n",
            "Epoch 14: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.8158\n",
            "Epoch 15/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7950\n",
            "Epoch 15: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7949\n",
            "Epoch 16/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7753\n",
            "Epoch 16: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7753\n",
            "Epoch 17/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7571\n",
            "Epoch 17: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7571\n",
            "Epoch 18/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7391\n",
            "Epoch 18: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7391\n",
            "Epoch 19/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7218\n",
            "Epoch 19: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7219\n",
            "Epoch 20/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.7062\n",
            "Epoch 20: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.7061\n",
            "Epoch 21/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6926\n",
            "Epoch 21: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6926\n",
            "Epoch 22/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6782\n",
            "Epoch 22: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6782\n",
            "Epoch 23/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6658\n",
            "Epoch 23: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6658\n",
            "Epoch 24/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6488\n",
            "Epoch 24: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6488\n",
            "Epoch 25/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6389\n",
            "Epoch 25: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6389\n",
            "Epoch 26/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6289\n",
            "Epoch 26: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6289\n",
            "Epoch 27/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6164\n",
            "Epoch 27: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6164\n",
            "Epoch 28/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.6070\n",
            "Epoch 28: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.6070\n",
            "Epoch 29/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5964\n",
            "Epoch 29: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5964\n",
            "Epoch 30/50\n",
            "1804/1806 [============================>.] - ETA: 0s - loss: 1.5870\n",
            "Epoch 30: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5871\n",
            "Epoch 31/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5769\n",
            "Epoch 31: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5769\n",
            "Epoch 32/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5692\n",
            "Epoch 32: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5692\n",
            "Epoch 33/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5597\n",
            "Epoch 33: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 60s 33ms/step - loss: 1.5597\n",
            "Epoch 34/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5527\n",
            "Epoch 34: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5527\n",
            "Epoch 35/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5429\n",
            "Epoch 35: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5429\n",
            "Epoch 36/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5380\n",
            "Epoch 36: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5380\n",
            "Epoch 37/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5285\n",
            "Epoch 37: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5285\n",
            "Epoch 38/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5231\n",
            "Epoch 38: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 59s 32ms/step - loss: 1.5232\n",
            "Epoch 39/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5147\n",
            "Epoch 39: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5146\n",
            "Epoch 40/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5111\n",
            "Epoch 40: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5112\n",
            "Epoch 41/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.5021\n",
            "Epoch 41: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.5020\n",
            "Epoch 42/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.4986\n",
            "Epoch 42: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.4986\n",
            "Epoch 43/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.4944\n",
            "Epoch 43: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.4944\n",
            "Epoch 44/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 1.4875\n",
            "Epoch 44: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 1.4875\n",
            "Epoch 45/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.4537\n",
            "Epoch 45: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.4538\n",
            "Epoch 46/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.7775\n",
            "Epoch 46: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.7775\n",
            "Epoch 47/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.6194\n",
            "Epoch 47: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.6194\n",
            "Epoch 48/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.5457\n",
            "Epoch 48: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.5457\n",
            "Epoch 49/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.4948\n",
            "Epoch 49: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.4948\n",
            "Epoch 50/50\n",
            "1805/1806 [============================>.] - ETA: 0s - loss: 2.4498\n",
            "Epoch 50: loss did not improve from 0.61487\n",
            "1806/1806 [==============================] - 58s 32ms/step - loss: 2.4498\n",
            "Выполнялось:48.36 минут\n",
            "Seed: \n",
            "\" g,\n",
            "     from ball to bed behold him come;\n",
            "     while petersburg's already rousing,\n",
            "     untirable, a \"\n",
            "==============================\n",
            "n the the the the theee\n",
            "     an the the the the the the the theee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an the the the the the the the seeee\n",
            "     an th\n",
            "Done\n"
          ]
        }
      ]
    }
  ]
}